{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file\n",
    "import datetime # to split the data into train and test\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import seaborn as sns # for plotting\n",
    "import numpy as np # for mathematical operations\n",
    "import warnings # to reduce warnings if needed\n",
    "from sklearn.tree import DecisionTreeClassifier # for building decision tree model\n",
    "from sklearn.ensemble import RandomForestClassifier # for building random forest model\n",
    "from xgboost import XGBClassifier # for building XGBoost model\n",
    "from sklearn.model_selection import GridSearchCV # for parameter tuning\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix # for model evaluation\n",
    "from sklearn.model_selection import train_test_split # for splitting the data into two sets; validation and training\n",
    "import random\n",
    "random.seed(2512)\n",
    "np.random.seed(2512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = \"Project/Data/match_data\"\n",
    "\n",
    "df = pd.read_csv(data_loc)\n",
    "\n",
    "mask = (df[[\"suspended\",\"stopped\"]] == False).all(axis=1) # mask for rows where all columns are False\n",
    "df = df[mask] # remove rows where all columns are False\n",
    "df = df.drop([\"suspended\",\"stopped\"], axis=1) # remove the columns\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_counts = df['result'].value_counts() # count the number of each result\n",
    "\n",
    "result_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.isna(): \n",
    "    if df[col].isna().sum() > 0:\n",
    "        print(\"{col} has {n} missing values\".format(col=col, n=df[col].isna().sum())) # print the number of missing values in each column\n",
    "\n",
    "# impute missing values with the mean of the column\n",
    "for column in df.columns:\n",
    "    if column != 'current_state' and df[column].isnull().sum() > 0: # if the column is not current_state since it is categorical\n",
    "        df[column] = df.groupby('fixture_id', observed=True)[column].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# forward fill the current_state column, since it is categorical\n",
    "df['current_state'] = df.groupby('fixture_id')['current_state'].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.isna():\n",
    "    if df[col].isna().sum() > 0:\n",
    "        print(\"{col} has {n} missing values\".format(col=col, n=df[col].isna().sum())) # print the number of missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"current_state\"]) # drop rows where current_state is missing\n",
    "\n",
    "df[\"current_state\"] = [1 if x == \"1\" else 2 if x == \"2\" else 0 for x in df[\"current_state\"]] # convert the current_state column to 1, 2, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns as a batch\n",
    "new_columns = pd.DataFrame({\n",
    "    \"Total Goals\": df[\"Goals - home\"] + df[\"Goals - away\"],\n",
    "    \"Total Red Cards\": df[\"Redcards - home\"] + df[\"Redcards - away\"],\n",
    "    \"Total Yellow Cards\": df[\"Yellowcards - home\"] + df[\"Yellowcards - away\"],\n",
    "    \"Total Injuries\": df[\"Injuries - home\"] + df[\"Injuries - away\"],\n",
    "    \"Total Substitutions\": df[\"Substitutions - home\"] + df[\"Substitutions - away\"],\n",
    "    \"Goal Difference\": df[\"Goals - home\"] - df[\"Goals - away\"],\n",
    "    \"Dangerous Attacks Difference\": df[\"Dangerous Attacks - home\"] - df[\"Dangerous Attacks - away\"],\n",
    "    \"Ball Possession % Difference\": df[\"Ball Possession % - home\"] - df[\"Ball Possession % - away\"],\n",
    "    \"Goal Attempts Difference\": df[\"Goal Attempts - home\"] - df[\"Goal Attempts - away\"],\n",
    "    \"Successful Passes % Difference\": df[\"Successful Passes Percentage - home\"] - df[\"Successful Passes Percentage - away\"]\n",
    "})\n",
    "\n",
    "# Concatenate the new columns with the original DataFrame\n",
    "df = pd.concat([df, new_columns], axis=1)\n",
    "\n",
    "# Add probabilities of home win, away win and draw\n",
    "df_odds = df.copy()\n",
    "df_odds = df_odds.loc[:, \"1\":\"X\"]\n",
    "prob_df = 1 / df_odds\n",
    "prob_df.columns = [\"Pr{Home Win}\", \"Pr{Away Win}\", \"Pr{Draw}\"]\n",
    "normalized_prob_df = prob_df.div(prob_df.sum(axis=1), axis=0)\n",
    "df = pd.concat([df, normalized_prob_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal in first 10 mins\n",
    "df[\"Goal_in_first_10_mins\"] = (df[\"halftime\"] == \"1st-half\") & (df[\"minute\"] < 10) & (df[\"Total Goals\"] >= 1)\n",
    "\n",
    "# Prepare matches dataframe for \"Goal_after_80_mins\"\n",
    "matches = df[(df[\"minute\"] < 40) & (df[\"halftime\"] == \"2nd-half\")]\n",
    "matches = matches.loc[matches.groupby(\"fixture_id\")[\"Total Goals\"].idxmax()]\n",
    "matches = matches[[\"fixture_id\", \"Total Goals\"]]\n",
    "\n",
    "# Goal after 80 mins\n",
    "df = df.merge(matches, on=\"fixture_id\", how=\"left\", suffixes=(\"\", \"_max\"))\n",
    "df[\"Goal_after_80_mins\"] = (df[\"minute\"] >= 40) & (df[\"halftime\"] == \"2nd-half\") & (df[\"Total Goals\"] > df[\"Total Goals_max\"])\n",
    "df.drop(columns=[\"Total Goals_max\"], inplace=True)\n",
    "\n",
    "# Red card in first 15 mins\n",
    "df[\"Redcard_in_first_15_mins\"] = (df[\"halftime\"] == \"1st-half\") & (df[\"minute\"] < 15) & (df[\"Total Red Cards\"] >= 1)\n",
    "\n",
    "# Prepare matches dataframe for \"Red_cards_after_75_mins\"\n",
    "matches = df[(df[\"minute\"] < 35) & (df[\"halftime\"] == \"2nd-half\")]\n",
    "matches = matches.loc[matches.groupby(\"fixture_id\")[\"Total Red Cards\"].idxmax()]\n",
    "matches = matches[[\"fixture_id\", \"Total Red Cards\"]]\n",
    "\n",
    "# Red cards after 75 mins\n",
    "df = df.merge(matches, on=\"fixture_id\", how=\"left\", suffixes=(\"\", \"_max\"))\n",
    "df[\"Red_cards_after_75_mins\"] = (df[\"minute\"] >= 35) & (df[\"halftime\"] == \"2nd-half\") & (df[\"Total Red Cards\"] > df[\"Total Red Cards_max\"])\n",
    "df.drop(columns=[\"Total Red Cards_max\"], inplace=True)\n",
    "\n",
    "# Yellow card in first 10 mins\n",
    "df[\"Yellowcard_in_first_10_mins\"] = (df[\"halftime\"] == \"1st-half\") & (df[\"minute\"] < 10) & (df[\"Total Yellow Cards\"] >= 1)\n",
    "\n",
    "# Injuries in first 15 mins\n",
    "df[\"Injuries_in_first_15_mins\"] = (df[\"halftime\"] == \"1st-half\") & (df[\"minute\"] < 15) & (df[\"Total Injuries\"] >= 1)\n",
    "\n",
    "# Substitutions in first 30 mins\n",
    "df[\"Substutions_in_first_30_mins\"] = (df[\"halftime\"] == \"1st-half\") & (df[\"minute\"] < 30) & (df[\"Total Substitutions\"] >= 1)\n",
    "\n",
    "# List of columns to check\n",
    "columns_to_check = [\n",
    "    \"Goal_in_first_10_mins\",\n",
    "    \"Redcard_in_first_15_mins\",\n",
    "    \"Yellowcard_in_first_10_mins\",\n",
    "    \"Injuries_in_first_15_mins\",\n",
    "    \"Goal_after_80_mins\",\n",
    "    \"Substutions_in_first_30_mins\",\n",
    "    \"Red_cards_after_75_mins\"\n",
    "]\n",
    "\n",
    "# Print event counts\n",
    "for col in columns_to_check:\n",
    "    print(f\"Number of {col} events: {df[col].sum()}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Current number of rows: {len(df)}\")\n",
    "\n",
    "# Remove rows with any of the specified events\n",
    "df = df[~df[columns_to_check].any(axis=1)]\n",
    "df = df.drop(columns=columns_to_check)\n",
    "\n",
    "# Print updated row count\n",
    "print(f\"Number of rows after removing matches with events: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'match_start_datetime' column to a date object and store it in a new column 'match_date'\n",
    "df['match_date'] = pd.to_datetime(df['match_start_datetime']).dt.date\n",
    "\n",
    "# Define the split date for training and test sets\n",
    "test_train_split_date = datetime.date(2024, 11, 1)\n",
    "\n",
    "# Create the test set: Rows where the match date is on or after the split date\n",
    "test_df = df[df['match_date'] >= test_train_split_date].copy()\n",
    "\n",
    "# Create the training set: Rows where the match date is before the split date\n",
    "train_df = df[df['match_date'] < test_train_split_date].copy()\n",
    "\n",
    "# Print the number of rows and unique matches in the training set\n",
    "print(f\"Number of rows in the training set: {len(train_df)}\")\n",
    "print(f\"Number of matches in the training set: {train_df['fixture_id'].nunique()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print the number of rows and unique matches in the test set\n",
    "print(f\"Number of rows in the test set: {len(test_df)}\")\n",
    "print(f\"Number of matches in the test set: {test_df['fixture_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define unrelated columns\n",
    "unrelated_columns = [\n",
    "    \"fixture_id\", \"current_time\", \"half_start_datetime\", \n",
    "    \"match_start_datetime\", \"latest_bookmaker_update\", \"name\", \n",
    "    \"result\", \"match_date\", \"final_score\"]\n",
    "\n",
    "# Prepare the full training set\n",
    "X_full_train = train_df.copy()\n",
    "X_full_train = X_full_train.drop(columns=unrelated_columns)\n",
    "X_full_train = pd.get_dummies(X_full_train, drop_first=True)\n",
    "\n",
    "y_full_train = np.select(\n",
    "    [train_df['result'] == \"1\", train_df['result'] == \"2\"],\n",
    "    [1, 2],\n",
    "    default=0\n",
    ")\n",
    "\n",
    "# Split the full training set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full_train, y_full_train, test_size=0.3, stratify=y_full_train, random_state = 2512\n",
    ")\n",
    "\n",
    "# Prepare the test set\n",
    "X_test = test_df.copy()\n",
    "X_test = X_test.drop(columns=unrelated_columns)\n",
    "X_test = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "y_test = np.select(\n",
    "    [test_df['result'] == \"1\", test_df['result'] == \"2\"],\n",
    "    [1, 2],\n",
    "    default=0)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Number of rows in the training set (split): {len(X_train)}\")\n",
    "print(f\"Number of rows in the validation set: {len(X_val)}\")\n",
    "print(f\"Number of rows in the test set: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Combination for Decision Tree\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],         \n",
    "    'max_depth': [None, 10, 20],          \n",
    "    'min_samples_split': [2, 5, 10],          \n",
    "    'min_samples_leaf': [1, 2, 5],            \n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'class_weight': ['balanced', None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best parameters for the Decision Tree model\n",
    "dt = DecisionTreeClassifier(random_state=2512)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=dt, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=10, \n",
    "                           n_jobs=-1, \n",
    "                           return_train_score=True,\n",
    "                           verbose=1)\n",
    "\n",
    "grid_search.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dt = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Predict on the training set and test set using the best model\n",
    "train_predictions = best_dt.predict(X_full_train)\n",
    "test_predictions = best_dt.predict(X_test)\n",
    "\n",
    "# Calculate the accuracies\n",
    "train_accuracy_dt = accuracy_score(y_full_train, train_predictions)\n",
    "test_accuracy_dt = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "# Print the best parameters and accuracies\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Training Accuracy: {train_accuracy_dt:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_dt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search.cv_results_\n",
    "params = results['params']\n",
    "mean_test_accuracy = results['mean_test_score']\n",
    "mean_train_accuracy = results['mean_train_score'] \n",
    "\n",
    "# Create a DataFrame with the results\n",
    "results_df = pd.DataFrame(params)\n",
    "results_df['mean_test_accuracy'] = mean_test_accuracy\n",
    "results_df['mean_train_accuracy'] = mean_train_accuracy\n",
    "\n",
    "results_df = results_df.sort_values(by='mean_train_accuracy', ascending=False)\n",
    "results_df = results_df.reset_index(drop=True)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(results_df['mean_test_accuracy'], label='Test Accuracy', color='orange')\n",
    "plt.plot(results_df['mean_train_accuracy'], label='Train Accuracy', color='blue')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Model')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importances = best_dt.feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_val.columns,  \n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "feature_importance_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "top_features = feature_importance_df.head(10)\n",
    "\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x='Importance', \n",
    "    y='Feature', \n",
    "    data=top_features, \n",
    "    hue=None,   \n",
    "    color='blue'  \n",
    ")\n",
    "\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_full_train, train_predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Draw\",\"Home Win\", \"Away Win\"], yticklabels=[\"Draw\",\"Home Win\", \"Away Win\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix of Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Draw\",\"Home Win\", \"Away Win\"], yticklabels=[\"Draw\",\"Home Win\", \"Away Win\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix of Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba_dt = best_dt.predict_proba(X_test) # Get the probabilities of the predictions\n",
    "\n",
    "# Create a DataFrame with the probabilities, real results, predicted results, fixture_id and odds\n",
    "betting_df = pd.DataFrame(predict_proba_dt, columns=[f'{i}' for i in range(predict_proba_dt.shape[1])])\n",
    "betting_df[\"real_result\"] = y_test\n",
    "betting_df[\"predicted_result\"] = test_predictions\n",
    "betting_df[\"fixture_id\"] = test_df[\"fixture_id\"].values\n",
    "betting_df[\"Odd_Home\"] = test_df[\"1\"].values\n",
    "betting_df[\"Odd_Away\"] = test_df[\"2\"].values\n",
    "betting_df[\"Odd_Draw\"] = test_df[\"X\"].values\n",
    "\n",
    "# Iterate over the rows and bet based on the probabilities\n",
    "betted_rows_dt_1 = {}\n",
    "\n",
    "for fixture_id, match in betting_df.groupby(\"fixture_id\"):\n",
    "    for id_, row in match.iterrows():\n",
    "        \n",
    "        if row[\"0\"] > 0.33:\n",
    "            betted_rows_dt_1[fixture_id] = {\"bet\": \"Draw\", \"odd\": row[\"Odd_Draw\"]}\n",
    "            break\n",
    "        elif row[\"1\"] > 0.33:\n",
    "            betted_rows_dt_1[fixture_id] = {\"bet\": \"Home Win\", \"odd\": row[\"Odd_Home\"]}\n",
    "            break\n",
    "        elif row[\"2\"] > 0.33:\n",
    "            betted_rows_dt_1[fixture_id] = {\"bet\": \"Away Win\", \"odd\": row[\"Odd_Away\"]}\n",
    "            break\n",
    "\n",
    "# Calculate the total return\n",
    "total_return_dt_1 = 0\n",
    "\n",
    "for fixture_id, betting_detail in betted_rows_dt_1.items():\n",
    "\n",
    "    real_result = betting_df[betting_df[\"fixture_id\"] == fixture_id][\"real_result\"].values[0]\n",
    "    odd = betting_detail[\"odd\"]\n",
    "    bet = betting_detail[\"bet\"]\n",
    "\n",
    "    if bet == \"Draw\" and real_result == 0:\n",
    "        total_return_dt_1 += odd\n",
    "    elif bet == \"Home Win\" and real_result == 1:\n",
    "        total_return_dt_1 += odd\n",
    "    elif bet == \"Away Win\" and real_result == 2:\n",
    "        total_return_dt_1 += odd\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "profit_dt1 = total_return_dt_1 - len(betted_rows_dt_1)\n",
    "print(f\"{total_return_dt_1:.2f} units of earn\")\n",
    "print(f\"{profit_dt1:.2f} units of profit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba_dt = best_dt.predict_proba(X_test) # Get the probabilities of the predictions\n",
    "\n",
    "# Create a DataFrame with the probabilities, real results, predicted results, fixture_id and odds\n",
    "betting_df = pd.DataFrame(predict_proba_dt, columns=[f'{i}' for i in range(predict_proba_dt.shape[1])])\n",
    "betting_df[\"real_result\"] = y_test\n",
    "betting_df[\"predicted_result\"] = test_predictions\n",
    "betting_df[\"fixture_id\"] = test_df[\"fixture_id\"].values\n",
    "betting_df[\"Odd_Home\"] = test_df[\"1\"].values\n",
    "betting_df[\"Odd_Away\"] = test_df[\"2\"].values\n",
    "betting_df[\"Odd_Draw\"] = test_df[\"X\"].values\n",
    "\n",
    "# Iterate over the rows and bet based on the probabilities\n",
    "betted_rows_dt_2 = {}\n",
    "\n",
    "for fixture_id, match in betting_df.groupby(\"fixture_id\"):\n",
    "    for id_, row in match.iterrows():\n",
    "        \n",
    "        if row[\"0\"] > 0.5:\n",
    "            betted_rows_dt_2[fixture_id] = {\"bet\": \"Draw\", \"odd\": row[\"Odd_Draw\"]}\n",
    "            break\n",
    "        elif row[\"1\"] > 0.5:\n",
    "            betted_rows_dt_2[fixture_id] = {\"bet\": \"Home Win\", \"odd\": row[\"Odd_Home\"]}\n",
    "            break\n",
    "        elif row[\"2\"] > 0.5:\n",
    "            betted_rows_dt_2[fixture_id] = {\"bet\": \"Away Win\", \"odd\": row[\"Odd_Away\"]}\n",
    "            break\n",
    "\n",
    "# Calculate the total return\n",
    "total_return_dt_2 = 0\n",
    "\n",
    "for fixture_id, betting_detail in betted_rows_dt_2.items():\n",
    "\n",
    "    real_result = betting_df[betting_df[\"fixture_id\"] == fixture_id][\"real_result\"].values[0]\n",
    "    odd = betting_detail[\"odd\"]\n",
    "    bet = betting_detail[\"bet\"]\n",
    "\n",
    "    if bet == \"Draw\" and real_result == 0:\n",
    "        total_return_dt_2 += odd\n",
    "    elif bet == \"Home Win\" and real_result == 1:\n",
    "        total_return_dt_2 += odd\n",
    "    elif bet == \"Away Win\" and real_result == 2:\n",
    "        total_return_dt_2 += odd\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "profit_dt2 = total_return_dt_2 - len(betted_rows_dt_2)\n",
    "print(f\"{total_return_dt_2:.2f} units of earn\")\n",
    "print(f\"{profit_dt2:.2f} units of profit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba_dt = best_dt.predict_proba(X_test) # Get the probabilities of the predictions\n",
    "\n",
    "# Create a DataFrame with the probabilities, real results, predicted results, fixture_id and odds\n",
    "betting_df = pd.DataFrame(predict_proba_dt, columns=[f'{i}' for i in range(predict_proba_dt.shape[1])])\n",
    "betting_df[\"real_result\"] = y_test\n",
    "betting_df[\"predicted_result\"] = test_predictions\n",
    "betting_df[\"fixture_id\"] = test_df[\"fixture_id\"].values\n",
    "betting_df[\"Odd_Home\"] = test_df[\"1\"].values\n",
    "betting_df[\"Odd_Away\"] = test_df[\"2\"].values\n",
    "betting_df[\"Odd_Draw\"] = test_df[\"X\"].values\n",
    "\n",
    "# Iterate over the rows and bet based on the probabilities\n",
    "betted_rows_dt_3 = {}\n",
    "\n",
    "for fixture_id, match in betting_df.groupby(\"fixture_id\"):\n",
    "    for id_, row in match.iterrows():\n",
    "        \n",
    "        if row[\"0\"] > 0.9:\n",
    "            betted_rows_dt_3[fixture_id] = {\"bet\": \"Draw\", \"odd\": row[\"Odd_Draw\"]}\n",
    "            break\n",
    "        elif row[\"1\"] > 0.9:\n",
    "            betted_rows_dt_3[fixture_id] = {\"bet\": \"Home Win\", \"odd\": row[\"Odd_Home\"]}\n",
    "            break\n",
    "        elif row[\"2\"] > 0.9:\n",
    "            betted_rows_dt_3[fixture_id] = {\"bet\": \"Away Win\", \"odd\": row[\"Odd_Away\"]}\n",
    "            break\n",
    "\n",
    "# Calculate the total return\n",
    "total_return_dt_3 = 0\n",
    "\n",
    "for fixture_id, betting_detail in betted_rows_dt_3.items():\n",
    "\n",
    "    real_result = betting_df[betting_df[\"fixture_id\"] == fixture_id][\"real_result\"].values[0]\n",
    "    odd = betting_detail[\"odd\"]\n",
    "    bet = betting_detail[\"bet\"]\n",
    "\n",
    "    if bet == \"Draw\" and real_result == 0:\n",
    "        total_return_dt_3 += odd\n",
    "    elif bet == \"Home Win\" and real_result == 1:\n",
    "        total_return_dt_3 += odd\n",
    "    elif bet == \"Away Win\" and real_result == 2:\n",
    "        total_return_dt_3 += odd\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "profit_dt3 = total_return_dt_3 - len(betted_rows_dt_3)\n",
    "print(f\"{total_return_dt_3:.2f} units of earn\")\n",
    "print(f\"{profit_dt3:.2f} units of profit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Combination for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50 ,100, 200],\n",
    "    'max_depth':  [5, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 5],\n",
    "    'n_jobs': [-1],\n",
    "    'class_weight': ['balanced', None]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best parameters for the Random Forest model\n",
    "rf = RandomForestClassifier(random_state=123)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=10, \n",
    "                           scoring='accuracy',\n",
    "                           return_train_score=True,\n",
    "                           n_jobs=-1, \n",
    "                           verbose=1)\n",
    "\n",
    "grid_search.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Predict on the training set and test set using the best model\n",
    "train_predictions = best_rf.predict(X_full_train)\n",
    "test_predictions = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracies\n",
    "train_accuracy_rf = accuracy_score(y_full_train, train_predictions)\n",
    "test_accuracy_rf = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "# Print the best parameters and accuracies\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Training Accuracy: {train_accuracy_rf:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(results_df['mean_test_accuracy'], label='Test Accuracy', color='orange')\n",
    "plt.plot(results_df['mean_train_accuracy'], label='Train Accuracy', color='blue')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Model')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = best_rf.feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_val.columns,  \n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "feature_importance_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "top_features = feature_importance_df.head(15)\n",
    "\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x='Importance', \n",
    "    y='Feature', \n",
    "    data=top_features, \n",
    "    hue=None,  \n",
    "    legend=False,  \n",
    "    color='blue'  \n",
    ")\n",
    "\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_full_train, train_predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Draw\",\"Home Win\", \"Away Win\"], yticklabels=[\"Draw\",\"Home Win\", \"Away Win\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix of Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Draw\",\"Home Win\", \"Away Win\"], yticklabels=[\"Draw\",\"Home Win\", \"Away Win\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix of Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba_rf = best_rf.predict_proba(X_test) # Get the probabilities of the predictions\n",
    "\n",
    "# Create a DataFrame with the probabilities, real results, predicted results, fixture_id and odds\n",
    "betting_df = pd.DataFrame(predict_proba_rf, columns=[f'{i}' for i in range(predict_proba_rf.shape[1])])\n",
    "betting_df[\"real_result\"] = y_test\n",
    "betting_df[\"predicted_result\"] = test_predictions\n",
    "betting_df[\"fixture_id\"] = test_df[\"fixture_id\"].values\n",
    "betting_df[\"Odd_Home\"] = test_df[\"1\"].values\n",
    "betting_df[\"Odd_Away\"] = test_df[\"2\"].values\n",
    "betting_df[\"Odd_Draw\"] = test_df[\"X\"].values\n",
    "\n",
    "# Iterate over the rows and bet based on the probabilities\n",
    "betted_rows_rf_1 = {}\n",
    "\n",
    "for fixture_id, match in betting_df.groupby(\"fixture_id\"):\n",
    "    for id_, row in match.iterrows():\n",
    "        \n",
    "        if row[\"0\"] > 0.33:\n",
    "            betted_rows_rf_1[fixture_id] = {\"bet\": \"Draw\", \"odd\": row[\"Odd_Draw\"]}\n",
    "            break\n",
    "        elif row[\"1\"] > 0.33:\n",
    "            betted_rows_rf_1[fixture_id] = {\"bet\": \"Home Win\", \"odd\": row[\"Odd_Home\"]}\n",
    "            break\n",
    "        elif row[\"2\"] > 0.33:\n",
    "            betted_rows_rf_1[fixture_id] = {\"bet\": \"Away Win\", \"odd\": row[\"Odd_Away\"]}\n",
    "            break\n",
    "\n",
    "# Calculate the total return\n",
    "total_return_rf_1 = 0\n",
    "\n",
    "for fixture_id, betting_detail in betted_rows_rf_1.items():\n",
    "\n",
    "    real_result = betting_df[betting_df[\"fixture_id\"] == fixture_id][\"real_result\"].values[0]\n",
    "    odd = betting_detail[\"odd\"]\n",
    "    bet = betting_detail[\"bet\"]\n",
    "\n",
    "    if bet == \"Draw\" and real_result == 0:\n",
    "        total_return_rf_1 += odd\n",
    "    elif bet == \"Home Win\" and real_result == 1:\n",
    "        total_return_rf_1 += odd\n",
    "    elif bet == \"Away Win\" and real_result == 2:\n",
    "        total_return_rf_1 += odd\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "profit_rf1 = total_return_rf_1 - len(betted_rows_rf_1)\n",
    "print(f\"{total_return_rf_1:.2f} units of earn\")\n",
    "print(f\"{profit_rf1:.2f} units of profit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba_dt = best_rf.predict_proba(X_test) # Get the probabilities of the predictions\n",
    "\n",
    "# Create a DataFrame with the probabilities, real results, predicted results, fixture_id and odds\n",
    "betting_df = pd.DataFrame(predict_proba_dt, columns=[f'{i}' for i in range(predict_proba_dt.shape[1])])\n",
    "betting_df[\"real_result\"] = y_test\n",
    "betting_df[\"predicted_result\"] = test_predictions\n",
    "betting_df[\"fixture_id\"] = test_df[\"fixture_id\"].values\n",
    "betting_df[\"Odd_Home\"] = test_df[\"1\"].values\n",
    "betting_df[\"Odd_Away\"] = test_df[\"2\"].values\n",
    "betting_df[\"Odd_Draw\"] = test_df[\"X\"].values\n",
    "\n",
    "# Iterate over the rows and bet based on the probabilities\n",
    "betted_rows_rf_2 = {}\n",
    "\n",
    "for fixture_id, match in betting_df.groupby(\"fixture_id\"):\n",
    "    for id_, row in match.iterrows():\n",
    "        \n",
    "        if row[\"0\"] > 0.5:\n",
    "            betted_rows_rf_2[fixture_id] = {\"bet\": \"Draw\", \"odd\": row[\"Odd_Draw\"]}\n",
    "            break\n",
    "        elif row[\"1\"] > 0.5:\n",
    "            betted_rows_rf_2[fixture_id] = {\"bet\": \"Home Win\", \"odd\": row[\"Odd_Home\"]}\n",
    "            break\n",
    "        elif row[\"2\"] > 0.5:\n",
    "            betted_rows_rf_2[fixture_id] = {\"bet\": \"Away Win\", \"odd\": row[\"Odd_Away\"]}\n",
    "            break\n",
    "\n",
    "# Calculate the total return\n",
    "total_return_rf_2 = 0\n",
    "\n",
    "for fixture_id, betting_detail in betted_rows_rf_2.items():\n",
    "\n",
    "    real_result = betting_df[betting_df[\"fixture_id\"] == fixture_id][\"real_result\"].values[0]\n",
    "    odd = betting_detail[\"odd\"]\n",
    "    bet = betting_detail[\"bet\"]\n",
    "\n",
    "    if bet == \"Draw\" and real_result == 0:\n",
    "        total_return_rf_2 += odd\n",
    "    elif bet == \"Home Win\" and real_result == 1:\n",
    "        total_return_rf_2 += odd\n",
    "    elif bet == \"Away Win\" and real_result == 2:\n",
    "        total_return_rf_2 += odd\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "profit_rf2 = total_return_rf_2 - len(betted_rows_rf_2)\n",
    "print(f\"{total_return_rf_2:.2f} units of earn\")\n",
    "print(f\"{profit_rf2:.2f} units of profit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba_dt = best_rf.predict_proba(X_test) # Get the probabilities of the predictions\n",
    "\n",
    "# Create a DataFrame with the probabilities, real results, predicted results, fixture_id and odds\n",
    "betting_df = pd.DataFrame(predict_proba_dt, columns=[f'{i}' for i in range(predict_proba_dt.shape[1])])\n",
    "betting_df[\"real_result\"] = y_test\n",
    "betting_df[\"predicted_result\"] = test_predictions\n",
    "betting_df[\"fixture_id\"] = test_df[\"fixture_id\"].values\n",
    "betting_df[\"Odd_Home\"] = test_df[\"1\"].values\n",
    "betting_df[\"Odd_Away\"] = test_df[\"2\"].values\n",
    "betting_df[\"Odd_Draw\"] = test_df[\"X\"].values\n",
    "\n",
    "# Iterate over the rows and bet based on the probabilities\n",
    "betted_rows_rf_3 = {}\n",
    "\n",
    "for fixture_id, match in betting_df.groupby(\"fixture_id\"):\n",
    "    for id_, row in match.iterrows():\n",
    "        \n",
    "        if row[\"0\"] > 0.9:\n",
    "            betted_rows_rf_3[fixture_id] = {\"bet\": \"Draw\", \"odd\": row[\"Odd_Draw\"]}\n",
    "            break\n",
    "        elif row[\"1\"] > 0.9:\n",
    "            betted_rows_rf_3[fixture_id] = {\"bet\": \"Home Win\", \"odd\": row[\"Odd_Home\"]}\n",
    "            break\n",
    "        elif row[\"2\"] > 0.9:\n",
    "            betted_rows_rf_3[fixture_id] = {\"bet\": \"Away Win\", \"odd\": row[\"Odd_Away\"]}\n",
    "            break\n",
    "\n",
    "# Calculate the total return\n",
    "total_return_rf_3 = 0\n",
    "\n",
    "for fixture_id, betting_detail in betted_rows_rf_3.items():\n",
    "\n",
    "    real_result = betting_df[betting_df[\"fixture_id\"] == fixture_id][\"real_result\"].values[0]\n",
    "    odd = betting_detail[\"odd\"]\n",
    "    bet = betting_detail[\"bet\"]\n",
    "\n",
    "    if bet == \"Draw\" and real_result == 0:\n",
    "        total_return_rf_3 += odd\n",
    "    elif bet == \"Home Win\" and real_result == 1:\n",
    "        total_return_rf_3 += odd\n",
    "    elif bet == \"Away Win\" and real_result == 2:\n",
    "        total_return_rf_3 += odd\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "profit_rf3 = total_return_rf_3 - len(betted_rows_rf_3)\n",
    "print(f\"{total_return_rf_3:.2f} units of earn\")\n",
    "print(f\"{profit_rf3:.2f} units of profit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best parameters for the XGBoost model\n",
    "xgb = XGBClassifier(random_state=123)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator = xgb,\n",
    "    param_grid = param_grid,\n",
    "    scoring = 'accuracy',\n",
    "    cv = 10,                \n",
    "    verbose = 1,\n",
    "    return_train_score = True)\n",
    "\n",
    "grid_search.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_bt = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Predict on the training set and test set using the best model\n",
    "train_predictions = best_bt.predict(X_full_train)\n",
    "test_predictions = best_bt.predict(X_test)\n",
    "\n",
    "# Calculate the accuracies\n",
    "train_accuracy_bt = accuracy_score(y_full_train, train_predictions)\n",
    "test_accuracy_bt = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "# Print the best parameters and accuracies\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Training Accuracy: {train_accuracy_bt:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_bt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = best_bt.feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_val.columns,  \n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "feature_importance_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "top_features = feature_importance_df.head(15)\n",
    "\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x='Importance', \n",
    "    y='Feature', \n",
    "    data=top_features, \n",
    "    hue=None,  \n",
    "    legend=False,  \n",
    "    color='blue'  \n",
    ")\n",
    "\n",
    "plt.title('Top 15 Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Draw\",\"Home Win\", \"Away Win\"], yticklabels=[\"Draw\",\"Home Win\", \"Away Win\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix of Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba_xgboost = best_bt.predict_proba(X_test) # Get the probabilities of the predictions\n",
    "\n",
    "# Create a DataFrame with the probabilities, real results, predicted results, fixture_id and odds\n",
    "betting_df = pd.DataFrame(predict_proba_xgboost, columns=[f'{i}' for i in range(predict_proba_xgboost.shape[1])])\n",
    "betting_df[\"real_result\"] = y_test\n",
    "betting_df[\"predicted_result\"] = test_predictions\n",
    "betting_df[\"fixture_id\"] = test_df[\"fixture_id\"].values\n",
    "betting_df[\"Odd_Home\"] = test_df[\"1\"].values\n",
    "betting_df[\"Odd_Away\"] = test_df[\"2\"].values\n",
    "betting_df[\"Odd_Draw\"] = test_df[\"X\"].values\n",
    "\n",
    "# Iterate over the rows and bet based on the probabilities\n",
    "betted_rows_bt_1 = {}\n",
    "\n",
    "for fixture_id, match in betting_df.groupby(\"fixture_id\"):\n",
    "    for id_, row in match.iterrows():\n",
    "        \n",
    "        if row[\"0\"] > 0.33:\n",
    "            betted_rows_bt_1[fixture_id] = {\"bet\": \"Draw\", \"odd\": row[\"Odd_Draw\"]}\n",
    "            break\n",
    "        elif row[\"1\"] > 0.33:\n",
    "            betted_rows_bt_1[fixture_id] = {\"bet\": \"Home Win\", \"odd\": row[\"Odd_Home\"]}\n",
    "            break\n",
    "        elif row[\"2\"] > 0.33:\n",
    "            betted_rows_bt_1[fixture_id] = {\"bet\": \"Away Win\", \"odd\": row[\"Odd_Away\"]}\n",
    "            break\n",
    "\n",
    "# Calculate the total return\n",
    "total_return_bt_1 = 0\n",
    "\n",
    "for fixture_id, betting_detail in betted_rows_bt_1.items():\n",
    "\n",
    "    real_result = betting_df[betting_df[\"fixture_id\"] == fixture_id][\"real_result\"].values[0]\n",
    "    odd = betting_detail[\"odd\"]\n",
    "    bet = betting_detail[\"bet\"]\n",
    "\n",
    "    if bet == \"Draw\" and real_result == 0:\n",
    "        total_return_bt_1 += odd\n",
    "    elif bet == \"Home Win\" and real_result == 1:\n",
    "        total_return_bt_1 += odd\n",
    "    elif bet == \"Away Win\" and real_result == 2:\n",
    "        total_return_bt_1 += odd\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "profit_bt1 = total_return_bt_1 - len(betted_rows_bt_1)\n",
    "print(f\"{total_return_bt_1:.2f} units of earn\")\n",
    "print(f\"{profit_bt1:.2f} units of profit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba_xgboost = best_bt.predict_proba(X_test) # Get the probabilities of the predictions\n",
    "\n",
    "# Create a DataFrame with the probabilities, real results, predicted results, fixture_id and odds\n",
    "betting_df = pd.DataFrame(predict_proba_xgboost, columns=[f'{i}' for i in range(predict_proba_xgboost.shape[1])])\n",
    "betting_df[\"real_result\"] = y_test\n",
    "betting_df[\"predicted_result\"] = test_predictions\n",
    "betting_df[\"fixture_id\"] = test_df[\"fixture_id\"].values\n",
    "betting_df[\"Odd_Home\"] = test_df[\"1\"].values\n",
    "betting_df[\"Odd_Away\"] = test_df[\"2\"].values\n",
    "betting_df[\"Odd_Draw\"] = test_df[\"X\"].values\n",
    "\n",
    "# Iterate over the rows and bet based on the probabilities\n",
    "betted_rows_bt_2 = {}\n",
    "\n",
    "for fixture_id, match in betting_df.groupby(\"fixture_id\"):\n",
    "    for id_, row in match.iterrows():\n",
    "        \n",
    "        if row[\"0\"] > 0.5:\n",
    "            betted_rows_bt_2[fixture_id] = {\"bet\": \"Draw\", \"odd\": row[\"Odd_Draw\"]}\n",
    "            break\n",
    "        elif row[\"1\"] > 0.5:\n",
    "            betted_rows_bt_2[fixture_id] = {\"bet\": \"Home Win\", \"odd\": row[\"Odd_Home\"]}\n",
    "            break\n",
    "        elif row[\"2\"] > 0.5:\n",
    "            betted_rows_bt_2[fixture_id] = {\"bet\": \"Away Win\", \"odd\": row[\"Odd_Away\"]}\n",
    "            break\n",
    "\n",
    "# Calculate the total return\n",
    "total_return_bt_2 = 0\n",
    "\n",
    "for fixture_id, betting_detail in betted_rows_bt_2.items():\n",
    "\n",
    "    real_result = betting_df[betting_df[\"fixture_id\"] == fixture_id][\"real_result\"].values[0]\n",
    "    odd = betting_detail[\"odd\"]\n",
    "    bet = betting_detail[\"bet\"]\n",
    "\n",
    "    if bet == \"Draw\" and real_result == 0:\n",
    "        total_return_bt_2 += odd\n",
    "    elif bet == \"Home Win\" and real_result == 1:\n",
    "        total_return_bt_2 += odd\n",
    "    elif bet == \"Away Win\" and real_result == 2:\n",
    "        total_return_bt_2 += odd\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "profit_bt2 = total_return_bt_2 - len(betted_rows_bt_2)\n",
    "print(f\"{total_return_bt_2:.2f} units of earn\")\n",
    "print(f\"{profit_bt2:.2f} units of profit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba_xgboost = best_bt.predict_proba(X_test) # Get the probabilities of the predictions\n",
    "\n",
    "# Create a DataFrame with the probabilities, real results, predicted results, fixture_id and odds\n",
    "betting_df = pd.DataFrame(predict_proba_xgboost, columns=[f'{i}' for i in range(predict_proba_xgboost.shape[1])])\n",
    "betting_df[\"real_result\"] = y_test\n",
    "betting_df[\"predicted_result\"] = test_predictions\n",
    "betting_df[\"fixture_id\"] = test_df[\"fixture_id\"].values\n",
    "betting_df[\"Odd_Home\"] = test_df[\"1\"].values\n",
    "betting_df[\"Odd_Away\"] = test_df[\"2\"].values\n",
    "betting_df[\"Odd_Draw\"] = test_df[\"X\"].values\n",
    "\n",
    "# Iterate over the rows and bet based on the probabilities\n",
    "betted_rows_bt_3 = {}\n",
    "\n",
    "for fixture_id, match in betting_df.groupby(\"fixture_id\"):\n",
    "    for id_, row in match.iterrows():\n",
    "        \n",
    "        if row[\"0\"] > 0.9:\n",
    "            betted_rows_bt_3[fixture_id] = {\"bet\": \"Draw\", \"odd\": row[\"Odd_Draw\"]}\n",
    "            break\n",
    "        elif row[\"1\"] > 0.9:\n",
    "            betted_rows_bt_3[fixture_id] = {\"bet\": \"Home Win\", \"odd\": row[\"Odd_Home\"]}\n",
    "            break\n",
    "        elif row[\"2\"] > 0.9:\n",
    "            betted_rows_bt_3[fixture_id] = {\"bet\": \"Away Win\", \"odd\": row[\"Odd_Away\"]}\n",
    "            break\n",
    "\n",
    "# Calculate the total return\n",
    "total_return_bt_3 = 0\n",
    "\n",
    "for fixture_id, betting_detail in betted_rows_bt_3.items():\n",
    "\n",
    "    real_result = betting_df[betting_df[\"fixture_id\"] == fixture_id][\"real_result\"].values[0]\n",
    "    odd = betting_detail[\"odd\"]\n",
    "    bet = betting_detail[\"bet\"]\n",
    "\n",
    "    if bet == \"Draw\" and real_result == 0:\n",
    "        total_return_bt_3 += odd\n",
    "    elif bet == \"Home Win\" and real_result == 1:\n",
    "        total_return_bt_3 += odd\n",
    "    elif bet == \"Away Win\" and real_result == 2:\n",
    "        total_return_bt_3 += odd\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "profit_bt3 = total_return_bt_3 - len(betted_rows_bt_3)\n",
    "print(f\"{total_return_bt_3:.2f} units of earn\")\n",
    "print(f\"{profit_bt3:.2f} units of profit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with the total return results\n",
    "earn_results = {\n",
    "    'Method': ['Decision Tree', 'Decision Tree', 'Decision Tree','Random Forest', 'Random Forest','Random Forest', 'Gradient Boosted Decision Tree', 'Gradient Boosted Decision Tree',  'Gradient Boosted Decision Tree'],\n",
    "    'Treshold': [0.33, 0.5, 0.9 ,0.33, 0.5, 0.9,0.33, 0.5, 0.9],\n",
    "    'Total Return': [total_return_dt_1, total_return_dt_2, total_return_dt_3, total_return_rf_1, total_return_rf_2, total_return_rf_3, total_return_bt_1, total_return_bt_2, total_return_bt_3],\n",
    "    'Total Played': [len(betted_rows_dt_1), len(betted_rows_dt_2), len(betted_rows_dt_3), len(betted_rows_rf_1), len(betted_rows_rf_2), len(betted_rows_rf_3), len(betted_rows_bt_1), len(betted_rows_bt_2), len(betted_rows_bt_3)],\n",
    "}\n",
    "\n",
    "earn_df = pd.DataFrame(earn_results)\n",
    "earn_df[\"Net Profit\"] = earn_df[\"Total Return\"] - earn_df[\"Total Played\"]\n",
    "\n",
    "earn_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
